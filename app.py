# app.py â€” Qwen3 åŒ»å­¦é—®ç­” Demo
import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# ========= é…ç½® =========
MODEL_PATH      = "./output/Qwen3-1.7B/checkpoint-1000"   # æƒé‡ç›®å½•
DEVICE          = "cuda" if torch.cuda.is_available() else "cpu"
MAX_NEW_TOKENS  = 1024
SYSTEM_MESSAGE  = "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚"

# ========= åŠ è½½æ¨¡å‹ =========
print("[INFO] Loading model â€¦")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, use_fast=False, trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto" if DEVICE == "cuda" else None,
    torch_dtype=torch.bfloat16 if DEVICE == "cuda" else torch.float32,
).eval()
print("[INFO] Model ready.")

# ========= æ¨ç†å›è°ƒ =========
@torch.inference_mode()
def generate(user_msg: str, history: list[tuple[str, str]]):
    """
    Parameters
    ----------
    user_msg : ç”¨æˆ·æœ¬æ¬¡è¾“å…¥
    history  : [('ä¸Šä¸€æ¡ user','ä¸Šä¸€æ¡ assistant'), ...]  â€” ChatInterface è‡ªåŠ¨ç»´æŠ¤
    Returns
    -------
    str      : æœ€æ–°åŠ©æ‰‹å›å¤ï¼ˆåªéœ€è¿™ä¸€æ¡ï¼‰
    """
    # 1. ç»„è£…å®Œæ•´æ¶ˆæ¯åºåˆ—
    messages = [{"role": "system", "content": SYSTEM_MESSAGE}]
    for u, a in history:
        messages.append({"role": "user", "content": u})
        if a:
            messages.append({"role": "assistant", "content": a})
    messages.append({"role": "user", "content": user_msg})

    # 2. æ„é€  prompt & æ¨ç†
    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer([prompt], return_tensors="pt").to(DEVICE)

    gen_ids = model.generate(
        **inputs,
        max_new_tokens=MAX_NEW_TOKENS,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )
    reply = tokenizer.decode(
        gen_ids[0, inputs.input_ids.shape[1]:],  # åªå–æ–°å¢ token
        skip_special_tokens=True,
    ).strip()

    # 3. ä»…è¿”å›åŠ©æ‰‹å›å¤
    return reply

# ========= UI =========
EXAMPLES = [
    "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘è¢«è¯Šæ–­ä¸ºç³–å°¿ç—…ï¼Œåº”è¯¥æ€æ ·ç®¡ç†é¥®é£Ÿä¸­çš„ç¢³æ°´åŒ–åˆç‰©ï¼Ÿ",
    "æ€»æ˜¯è…°ç—›ï¼Œæ€€ç–‘è‚¾ç»“çŸ³ï¼Œéœ€è¦åšå“ªäº›æ£€æŸ¥ï¼Ÿ",
    "å„¿ç«¥å‘çƒ§ 39 Â°C ä»¥ä¸Šï¼Œå®¶é•¿åº”å¦‚ä½•å¤„ç†ï¼Ÿ",
]

with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue")) as demo:
    gr.Markdown(
        """
        # ğŸ©º Qwen3 åŒ»å­¦é—®ç­”æ¼”ç¤º  
        åŸºäº SFT çš„ **Qwen3-1.7B**ï¼Œæ”¯æŒå¸¦ Chain-of-Thought çš„ä¸“ä¸šåŒ»å­¦å›ç­”ã€‚  
        <span style="color:#d9534f"><strong>å…è´£å£°æ˜</strong>ï¼šä»…ä¾›æŠ€æœ¯äº¤æµä¸å­¦æœ¯ç ”ç©¶ï¼Œä¸å¯æ›¿ä»£åŒ»ç–—è¯Šæ–­ã€‚</span>
        """,
        elem_id="title",
    )

    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                placeholder="<div style='text-align:center;'><strong>è¯·è¾“å…¥åŒ»å­¦é—®é¢˜â€¦</strong></div>"
            )
            gr.ChatInterface(
                fn=generate,
                chatbot=chatbot,
                examples=EXAMPLES,
                retry_btn="ğŸ”„ é‡æ–°ç”Ÿæˆ",
                undo_btn="â†©ï¸ æ’¤å›ä¸Šä¸€æ¡",
                clear_btn="ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯",
            )
        with gr.Column(scale=1, min_width=200):
            gr.Markdown("## ä½¿ç”¨è¯´æ˜")
            gr.Markdown(
                """
                1. åœ¨å·¦ä¾§è¾“å…¥åŒ»å­¦ç›¸å…³é—®é¢˜ã€‚\n
                2. æ¨¡å‹ä¼šè¿›è¡Œå†…éƒ¨æ€è€ƒå¹¶è¿”å›ç»“æ„åŒ–ç­”å¤ã€‚\n
                3. ç‚¹å‡»ã€Œæ¸…ç©ºå¯¹è¯ã€å¯é‡æ–°å¼€å§‹ã€‚\n
                è‹¥éœ€ä¿®æ”¹æ¨ç†å‚æ•°ï¼Œå¯ç›´æ¥ç¼–è¾‘ `app.py`ã€‚
                """
            )

    gr.Markdown("--- Â© 2025 xuxufei12 â€” Powered by Gradio 4 & Transformers")

if __name__ == "__main__":
    demo.launch(server_port=6006, share=True)
